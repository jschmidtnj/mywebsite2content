# Factoryrobot 2.0

This summer (2019) I somehow managed to get an internship at Bell Labs in Murray Hill. For those of you who don't know, Bell Labs is an international research facility known for inventing the first transistor, developing Unix, and winning many Nobel prizes in the process. Interning there is a big deal. I was part of the BR Lab, which stands for BHAG (big hairy audacious goals) Realization lab (super-acronym, I know), and it is therefore focused on the most futuristic, long-term moonshot projects. It is similar to Google's X Labs. Bell Labs was split from AT&T in 1996 (antitrust laws) and added to Lucent, later merging with Alcatel to become Alcatel-Lucent, and then this merged company was bought by Nokia in 2016. So now Bell Labs does research under the umbrella and funding of Nokia, which is not really a phone company anymore, positioning itself instead as a networking solutions company. Nokia is pioneering 5G technology and IoT infrastructure, and in turn a lot of the technology Bell Labs is researching is focused on internet-connected, high-bandwidth, 5G and cloud solutions. In the BR lab, I was working on an autonomous robot, a robot that can map and navigate the world around it, and additionally can interact with physical objects using a forklift or other modular devices. It was using the local 4G LTE and 5G infrastructure, in addition to WiFi, along with local "edge cloud" solutions, to offload most of the computation.

## Design Challenge

To illustrate how Nokia's 5G networking products can be beneficial for a future workforce, Bell Labs designed an elaborate demonstration area, to be also used for testing their designs. One part of the area was the factory, where robots would coordinate to move multi-colored boxes around using small forklifts. When I started, these robots were already operational, but they were using an insufficient software stack. The original team that made these robots was just trying to meet a deadline and did not design for them to be high-performance and cooperative. Instead the robots moved in a choreographed sequence that could not be changed without intense software engineering effort. The challenge was to upgrade the software to use the autonomous navigation stack used by other robots in the demo area, allowing a user to tell the robot to move to a certain location on the map, and the robot plans a route to get there.

## First Steps

The first thing we had to do was determine if this was a hardware or software problem. Initially, it looks like everything can be changed in software - all we needed to do was convert the robot from using the old software to the new software. However, the new software stack required a specific motorcontroller type to maintain consistency with the other robots in the demo. Then it turned out the power distribution system was inconsistent at best, and also needed to be upgraded. The Inertial Measurement Unit (IMU), central processor, rotor motors, all had to then be replaced for similar reasons. So it really turned out to be both a hardware and software problem.

## Swerve Drive

The main complication with this robot is that it is using swerve drive. Swerve drive requires 8 motors, 2 for each wheel, with of of those 2 for rotating the wheel and the other for driving the wheel. This allows for full kinematic motion, theoretically without slippage and a full 3 degrees of freedom (x, y, angular). This differentiated the robot from the others, which were all differential drive - tank drive without the treads. We wanted to preserve this aspect of the robots because it allows for superior motion and better results when doing odometry - motor movement feedback. However this requires changing the codebase and hardware to ensure that it is capable of that full motion. On the hardware side, there were two major changes that needed to be made - the rotor motors needed to be replaced because they were underpowered and kept breaking. And secondly the cables going to the drive motors for speed feedback - using encoders, and motor power needed to be managed as the wheels rotate around. The original manufacturing team did not manage the cables and instead had the software people limit the motion to movement in the x direction or spinning in place, but we wanted to allow for a combination of movements in the x, y, and omega (angular) direction. Therefore cable management would be a big issue.

On the software side, the whole stack that is hardware specific needed to be changed to account for the 4 motorcontrollers (instead of just 1) that we would be using (2 per motor), the new math involved for fully kinematic motion along all 3 axes, and the feedback for all of these motors (not to mention numerous other things). I was working on the software side so I will mainly go over this.

## Software

Whenever you have a big challenge ahead of you, you always break it down into simple things. The first goal in this case was just to get the brains of the operation - an Intel Upboard (x86-64 chip) connected and talking to a single Roboteq motorcontroller. Once I got a simple query working, I created a SerialService class for communicating with serial devices and then a SerialMotorController class to specifically talk to a motorcontroller (inheriting the base SerialService class). I was working with Python in ROS (Robot Operating System), and decided to use threads so that the serial writes and reads were non-blocking. With these two classes finished, I created a third class called MotorController for using the methods in the SerialMotorController class to perform specific operations, such as updating the current motor fault time or recording the current rpm. The next step was to create a FactoryRobot class instantiating all of these motorcontrollers and doing complex logic with all of this data.

### ROS

We were using robot operating system for handling all of the message data. ROS is a misnomer - it is not an operating system at all, but instead a framework for connecting a robot together and to the cloud. It works in a publish-subscribe model, where the different nodes, such as the IMU, publish to a topic (`/imu`), which anyone in the network can then subscribe to to use the data associated (navigation in the cloud for example). The robot needed to publish all the correct topics so that the cloud side could subscribe to the topics it needs to navigate and interpret the current state of the robot. The most important topic is command velocity, which was used by move-base to tell the robot at what velocity it should move at any given time in the three degrees of freedom. Then using the feedback from the odometry topic, the navigation is then able to recalculate how fast the robot should move at the next cycle.

### Kinematics

At the heart of this lies kinematics. Inverse kinematics was used to calculate given a vx, vy and omega, the necessary speed and angle for each wheel. This speed would then be converted to a number between -1000-1000 - the units for the motorcontroller. Then this data would be sent to the wheel. I will not go into the math too detailed here, but essentially given the moment arm and angular velocity, you are able to scale the angular velocity to components at each wheel. You are also able to find the components of vx and vy at each wheel. You can then use this data to find the aggregated components for vx and vy at each wheel, and using trigonometry you can find the angle. Forward kinematics is then used to calculate the feedback - the actual vx, vy and omega travelled, given the motion at each wheel - the current angles and speeds. Again, I won't go into details but essentially you can use trig to find the vx and vy components at each wheel, and then average them to find the combined vx and vy. Omega can be found by calculating the point of intersection of lines normal to each of the wheels, and multiplying the distance from that point to the center of the robot by the linear velocity. This omega is over constrained, so I was able to use linear regression to get a good approximation of what omega actually is. Additionally, I calculate vx, vy and omega from the IMU, so I was able to create a common filter between the IMU data and the encoder data (speeds and angles are coming from encoders) to determine the most accurate odometry possible.

## Other things

Once all of the math was figured out, there was not much more to do on the software side besides troubleshooting, ensuring it works with ROS and the rest of the stack, and actually getting the robot moving correctly. On the hardware side, once the improved power distribution was done, the rotor motors needed to be replaced, the wiring and cable management for the drive motors was next, and the motorcontrollers after that. We additionally wanted to add a break-out panel on the top of the robot to allow for easy access and modification, consisting of 5V, 12V, and 9V barrel plugs, usb and ethernet. Additionally we wanted to make the lidar and camera easily replaceable, and we wanted to make the front end effector modular, so that the forklift could be replaced with something different. We used ROS serial to accomplish this, so that when the arduino controlling the forklift is plugged in it would publish and subscribe to relevant ROS topics. Different from, for example, a robotic arm.

## Last thoughts

As you can tell, I had a lot of fun working on this robot and in the end we got fairly good results. The robot was able to maneuver in all 3 degrees of freedom it was designed for, and its movement was accurate to the feedback. The navigation worked out of the box, with the robot successfully navigating around the lab and avoiding obstacles. There is still some tuning work to be done with the pid loop for the drive motors, and some additional cable management may be necessary, but otherwise everything seems to be working well. I wanted to give a shout-out to Ilija, my manager, and my co-workers Anthony and Andrew, for working on this project with me and allowing for it to be successful. Somehow we even received the Innovation Project Award for our work, which I hope means that our project will be useful for years to come, continuing the research in autonomous robots for use in factories and on roads and throughout our lives.
